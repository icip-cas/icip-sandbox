# Code Sandbox

A secure sandbox for running and judging code generated by LLMs.

## Table of Contents
- [Updates](#updates)
- [Features](#features)
- [Contributing](#contributing)
- [License](#license)

## Updates
- Added support for unified evaluation interface for code generation tasks.
- Improved sandbox setup parameters for better performance and flexibility.
- Added support for distributed deployment

## Features

**Code Runner**: Run and return the result of a code snippet

Supported languages:

- Python (python, pytest)
- C++
- C#
- Go (go, go test)
- Java (javac, junit)
- NodeJS
- Typescript (tsx, jest)
- Scala
- Kotlin
- PHP
- Rust
- Bash
- Lua
- R
- Perl
- D
- Ruby
- Julia
- Verilog
- CUDA (GPU)
- Python (GPU)

Jupyter mode kernels:

- python3

**Online Judge**: Implementation of Evaluation & RL datasets that requires code running

- [HumanEval](https://github.com/openai/human-eval)
- [MultiPL-E HumanEval](https://github.com/nuprl/MultiPL-E)
- [Shadow Humaneval](https://huggingface.co/datasets/Miaosen/openai-humaneval-sky-shadow)
- [CodeContests](https://github.com/google-deepmind/code_contests)
- [MBPP](https://github.com/google-research/google-research/tree/master/mbpp)
- [MBXP](https://github.com/amazon-science/mxeval)
- [MHPP](https://github.com/SparksofAGI/MHPP)
- [CRUXEval](https://github.com/facebookresearch/cruxeval)
- [NaturalCodeBench](https://github.com/THUDM/NaturalCodeBench)
- [PAL-Math](https://github.com/deepseek-ai/DeepSeek-Coder/tree/main/Evaluation/PAL-Math)
- [verilog-eval](https://github.com/NVlabs/verilog-eval)

**Unified Evaluation**: A unified evaluation interface for code generation tasks, including stdio and function call evaluation modes on various languages

- [common_evaluate_batch](#usage)

## Contributing

### Installation

**Docker**

Use the provided docker `[Dockerhub URL]` 

Or, build the image locally:

```bash
docker build --rm -f ./scripts/Dockerfile.v2 -t code_sandbox:server .
```

### Deployment

To set up the sandbox service:

```bash
# To build the real docs, run `cd docs && npm ci && npm run build`
# Set parameters
export HOST=0.0.0.0
export PORT=8080
export WORKERS=4 # Number of parallel workers for uvicorn, set 1 for single CPU environment
export MAX_MEM=500000 # Maximum memory limit for each process in KB, set 500000 for 500MB, or 'unlimited' for no limit

# Start the server
make run-online
# Or `bash start_sandbox_with_supervisor.sh` for automatic restart on failure with Supervisor
```

Or set up multiple server in the distributed environment:
On the main node, set up nginx on the port `8081`
```bash
bash start_distributed.sh
```
On each worker node, run
```bash
export HOST=0.0.0.0
export PORT=8080
export WORKERS=4
export MAX_MEM=500000

make run-distributed
```

### Usage
Here is an example of how to use the `common_evaluate_batch` API for testing a+b problem with standard input/output format.
```python
# stdio evaluate
payload = {
  "completion": """```python\na, b = map(int, input().split())\nprint(a + b)\n```""",
    "config": {
        "language": "python",
        "run_timeout": 10,
        "provided_data": { 
            "test_cases": 
                {"type": "stdin_stdout", "input": ["1 2", "3 4"], "output": ["3", "7"], "fn_name": None},            
        },
        "extra": {
            "run_all_cases": True
        }
    }
}

response = requests.post('http://0.0.0.0:8080/common_evaluate_batch', json=payload)
result = response.json()
```
And the response would be:
```json
{
  "id": 0,
  "accepted": true,
  "extracted_code": "a, b = map(int, input().split())\nprint(a + b)",
  "full_code": null,
  "test_code": null,
  "tests": [
    {
      "passed": true,
      "exec_info": {
        "status": "Success",
        "message": "",
        "compile_result": null,
        "run_result": {
          "status": "Finished",
          "execution_time": 0.0040967464447021484,
          "return_code": 0,
          "stdout": "3\n",
          "stderr": ""
        },
        "executor_pod_name": null,
        "files": {}
      },
      "test_info": {
        "input": {
          "stdin": "1 2"
        },
        "output": {
          "stdout": "3"
        }
      }
    },
    {
      "passed": true,
      "exec_info": {
        "status": "Success",
        "message": "",
        "compile_result": null,
        "run_result": {
          "status": "Finished",
          "execution_time": 0.017037630081176758,
          "return_code": 0,
          "stdout": "7\n",
          "stderr": ""
        },
        "executor_pod_name": null,
        "files": {}
      },
      "test_info": {
        "input": {
          "stdin": "3 4"
        },
        "output": {
          "stdout": "7"
        }
      }
    }
  ],
  "extracted_type": null,
  "extra": null
}
```
Also an example of function call evaluation for the same problem:
```python
# function evaluate batch
payload = {
    "completion": """```python\ndef add(a, b):\n    return a + b\n```""",
    "config": {
        "language": "python",
        "provided_data": { 
            "test_cases": 
                {"type": "function_call", "input": [[1, 2], [3, 4]], "output": [3, 7], "fn_name": "add"},            
        },
        "extra": {
            "run_all_cases": True
        }
    }
}

response = requests.post('http://0.0.0.0:8080/common_evaluate_batch', json=payload)
result = response.json()
```
With the response:
```json
{
  "id": 0,
  "accepted": true,
  "extracted_code": "def add(a, b):\n    return a + b",
  "full_code": null,
  "test_code": null,
  "tests": [
    {
      "passed": true,
      "exec_info": {
        "status": "Success",
        "message": "",
        "compile_result": null,
        "run_result": {
          "status": "Finished",
          "execution_time": 0.00021147727966308594,
          "return_code": 0,
          "stdout": "",
          "stderr": ""
        },
        "executor_pod_name": null,
        "files": {}
      },
      "test_info": {
        "type": "function_call",
        "fn_name": "add",
        "input": [
          1,
          2
        ],
        "output": [
          3
        ]
      }
    },
    {
      "passed": true,
      "exec_info": {
        "status": "Success",
        "message": "",
        "compile_result": null,
        "run_result": {
          "status": "Finished",
          "execution_time": 0.01851511001586914,
          "return_code": 0,
          "stdout": "",
          "stderr": ""
        },
        "executor_pod_name": null,
        "files": {}
      },
      "test_info": {
        "type": "function_call",
        "fn_name": "add",
        "input": [
          3,
          4
        ],
        "output": [
          7
        ]
      }
    }
  ],
  "extracted_type": null,
  "extra": null
}
```


### Dev & Test

Refer to installation section for the setup of development environment.

Run all unit tests:

```bash
make test
```

Run a specific unit test (allows you to see stdout):

```bash
make test-case CASE=test_java_assert
```

Run a specific unit test with pdb:

```bash
make test-case-pdb CASE=test_java_assert
```

Format the code:

```bash
make format
```

<!-- ## License

```
Copyright 2024 Bytedance Ltd. and/or its affiliates

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
 -->
